% - Weak Scalability Design : Keep Pipeline of Ensembles to show barrier
%   needed in S5 and S6.
% - Performance, generality with weak scaling (agnostic to kernel) Added
%   functionality (do not speak about binding adaptivity to performance or
%   generality).
% - In use case: add in why TIES is challenging, and why adaptivity is
%   challenging.

\mtnote{General note. \\
        - Currently, this section is a merge of two distinct contributions.
          These need to be better merged. For example, we should consider
          whether (i) the description of the adaptive experiments should be
          moved to the first subsection ``Experimental Setup''; (ii) Table 1
          should be extended to include the adaptive/nonadaptive properties;
          (iii) the discussion of the results should have a uniform
          argumentative format; etc.\\
        - The structure of the section should be revise making the division
          in subsections consistent and changing/refining their naming.\\
        - A.0,1,2 require major rewriting of the text and the writing of
          relevant portions of new text. This includes: (i) eliminating
          redundant (copied) text; (ii) Eliminating or referencing text
          copied from other publications; (iii) reorganize the argumentation
          following the other general note left in the comments and better
          separating the experimental variables (especially including
          adaptive/nonadaptive) properties; (iv) adding plots (even if with
          incomplete results) and writing an actual analysis of the results
          for each plot.}


\jdnote{Revisions: \\ 
        - Rewrote experimental setup by starting with motivation and
          explaining overview of experiments
        - Appended all experiments to table
        - Created subsections for weak scaling, strong scaling, validation,
          adaptive experiments
        - Added plots (some are incomplete) and rearranged analysis
        - Kristof added his data on (06/03) for adaptive experiments and
          wrote results (adaptive plots to come Tuesday 06/05)
        - Strong scaling subsection still waiting on data from ESMACS
        - For now, removed anything related to Titan}

% ---------------------------------------------------------------------------
\subsection{Experiment Setup}\label{ssec:exp_design}

Computational drug campaigns require sizable compute resources % in order 
to run concurrent simulations for a large number of drug candidates.
Therefore, we perform our experiments on machines that can deliver at the
scale necessary to support these demands. \jdnote{should we mention anything
about exascale?}\mtnote{I would not but I would quantify the scale at which
we need to run. As it stands, this sentence appears too generic too me.}
Before embarking on a drug campaign that will utilize 150 million core hours
on NCSA Blue Waters, we perform experiments to characterize the weak and
strong scaling performance of HTBAC and its overheads on Blue Waters. We
validate the free energy calculation results produced using HTBAC by
comparing them with results previously cited \mtnote{what does this mean? We
cited the results already in the paper? These results are published?}
% not having used HTBAC
\mtnote{This sentence does not read well to me. I would refine, depending on
the answer to the previous comment}.

Given that protocols like TIES are more computationally demanding
\mtnote{Grammar: more than?}, it is paramount to use resource efficiently,
especially for campaigns that have a predefined computational budget. as
described in Section III\mtnote{please use LaTeX references} and IV
\mtnote{please use LaTeX references}, adaptive methods \mtnote{Methods for?
Maybe `simulation methods'?} have the potential for an higher computational
efficiency when compared to non-adaptive methods. Specifically, adaptive
methods can reduce the number of simulations for each lead candidate without
a loss in free energy accuracy and with a lower computational load. Further,
given a defined number of resources, adaptive methods can also increase
computational efficiency by improving the rate of statistical convergence.

Specifying, coordinating and executing adaptive \mtnote{simulation?} methods
requires system software supporting runtime decisions. As seen in
Section~\ref{sec:htbac}, HTBAC as been designed to enable decisions based on
intermediate runtime results. These results are evaluated to determine a
redistribution of resource to better support the execution of new
simulations. We use HTBAC to perform experiments about adaptivity, comparing
and characterizing resource efficiency with non-adaptive \mtnote{simulation?}
methods. \mtnote{Note: experiments are not adaptive, they are about adaptive
simulation methods. Analogously, experiments are not compared to non-adaptive
experiments but they (should) offer comparable data about both types of
methods.} \mtnote{Note: I rewrote the whole paragraph. The previous version
was compressed and ``left too much in the authors' pens''.}

% Commented section below will go in Section III and Section IV

% -------------------------------------------------------------------------- 
%
% Drug screening typically occurs in two phases. The first phase runs
% simulations of all drug candidates and determines which candidates become
% lead candidates. This phase is performed using less computationally
% intensive protocols such as ESMACS. Once we eliminate poor-binding
% candidates, we begin the second phase of the campaign where we investigate
% lead compounds using computationally more demanding protocols such as TIES.
% During the second phase, the accuracy of each candidate is evaluated. A
% high level of variability exists in the chemical properties across
% candidates. Therefore, the TIES protocols assumes the most general case in
% evaluating the binding free energy accuracy. To maintain generality, the
% TIES protocol requires 13 evenly spaced simulations, captured using a
% sampling parameter references as $\lambda$ windows in Section III. Although
% generality asserts the correct convergence criteria, this approximation of
% $\lambda$ windows induces higher computational costs as a result of
% executing large yet sometimes unnecessary simulations.

% On the other hand, adaptive methods such as the adaptive quadratures
% highlighted Section III, require less sampling and thereby reduce the
% number of simulations, without sacrificing accuracy. Because this method is
% tailored to every candidate, there is no way to understand \textit{a
% priori} the placement of $\lambda$ windows and thereby the total number of
% simulations.

% This requires the ability to instantiate simulations that predicate on
% intermediate results during runtime.

% Define parameters: added simulations means task count adaptivity and task
% attribute adaptivity. The next set (both number of resources) of
% simulations depends on the output properties of the previous simulations.
% When we adapt the number of simulations, the distribution of resources for
% each task within a static pilot will fluctuate.

% Carefully constructed $\lambda$ placement requires that the workflow can be
% adaptable by creating new computational tasks during execution, thereby
% modify the task graph during runtime. In HTBAC, construction of an adaptive
% workflow is predicated upon the workload management system (define) that
% provides expression of adaptive features.

% while communicating those parameters down to the runtime system (define).


% The post-execution properties are captured by the workload management
% system, EnTK.


% We then perform adaptive experiments to show improvements in
% time-to-convergence.

% -------------------------------------------------------------------------- %


\begin{table*}
    \caption{Parameters of scalability and adaptivity experiments.}
    \label{tab:experiments}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l            % Experiment ID
                    l            % experiment type
                    l            % physical system 
                    l            % CI
                    l            % protocol type
                    l            % number of protocols
                    l            % total cores
                    }
    %
    \toprule
    %
    \B{ID}                            &  % Experiment ID
    \B{Type of Experiment} &            % Adaptive or Scale
    \B{Physical System(s)} &            % Adaptive or Scale
    \B{Computing Infrastructure (CI)} &  % CI
    \B{Protocol(s)}                   &  % protocol type
    \B{No. Protocol(s)}                    &  % number of protocols
    \B{Total Cores}                     \\ % total cores
    % \B{Executable}                          \\ % executable
    %
    \midrule
    %
    \B{1}                             &  % Experiment ID
    Weak scaling                      &
    BRD4                              &
    Blue Waters                       &  % CI
    ESMACS                            &  % protocol type
    (2, 4, 8, 16)                     &  % number of protocols
    1600, 3200, 6400                  \\  % total cores
    % NAMD-MPI                               \\ % executable
    %
    % \B{2}                             &  % Experiment ID
    % Weak scaling                      &
    % BRD4                              &
    % Titan                          &  % CI
    % ESMACS                          &  % protocol type
    % (16, 64, 128)                    &  % number of protocols
    % 6400, 25600, 51200              &  % total cores
    % NAMD (non-MPI)                 \\ % executable
    % 
    \B{2}                             &  % Experiment ID
    Weak scaling                      &
    BRD4                              &
    Blue Waters                       &  % CI
    TIES                              &  % protocol type
    (2, 4, 8)                         &  % number of protocols
    4160, 8320, 16640                 \\  % total cores
    % NAMD-MPI                              \\ % executable
    % %
    % \B{4}                             &  % Experiment ID
    % Weak scaling                      &
    % BRD4                              &
    % Titan                          &  % CI
    % TIES                          &  % protocol type
    % (8, 32, 64)                    &  % number of protocols
    % 16640, 66560, 133120              &  % total cores
    % NAMD (non-MPI)                 \\ % executable
    %
    \B{3}                             &  % Experiment ID
    Weak scaling                      &
    BRD4                              &
    Blue Waters                       &  % CI
    ESMACS + TIES                     &  % protocol type
    (2, 4, 8)                    &  % number of protocols
    5280, 10560, 21120               \\ % total cores
    % NAMD-MPI                              \\ % executable
    %
    \B{4}                             &  % Experiment ID
    Strong scaling                      &
    BRD4                              &
    Blue Waters                          &  % CI
    TIES                          &  % protocol type
    (8, 8, 8)                    &  % number of protocols
    16640, 8320, 4160               \\ % total cores
    % NAMD-MPI                              \\ % executable
    %
    \B{5}                             &  % Experiment ID
    Strong scaling                          &  % CI
    BRD4                      &
    Blue Waters                              &
    ESMACS                          &  % protocol type
    (16, 16, 16)                    &  % number of protocols
    6400, 3200, 1600                \\ % total cores
    % NAMD-MPI                              \\ % executable
    %
    \B{6}                             &  % Experiment ID
    Non-adaptivity                       &  
    PTP1B, MC1, TYK2                     &  
    Blue Waters                          &  % CI                            
    TIES                          &  % protocol type
    (1, 1, 1)                           &  % number of protocols
    2080, 2080, 2080                       \\ % total cores
    %
    \B{7}                             &  % Experiment ID
    Adaptivity                       &  
    PTP1B, MC1, TYK2                  &  
    Blue Waters                          &  % CI                            
    TIES                          &  % protocol type
    (1, 1, 1)                           &  % number of protocols
    2080, 2080, 2080                           \\ % total cores
    % NAMD-MPI                              \\ % executable
    %
    \bottomrule
    %
    \end{tabular}
    }
\up{}
\end{table*}

\mtnote{The following paragraphs are disconnected from the previous ones. We
need a `joining' paragraph starting from something like: ``We designed n
experiments\ldots We used m protocols\ldots See Table o\ldots''. Moved a paragraph here as a starting point.} 
Experiment 1 measures the weak scalability of HTBAC, EnTK and RP using
multiple instances of the ESMACS protocols. Experiment 2 measures weak
scaling performance of TIES. Experiments 3 repeats the same design as
experiments 1 and 2 using both TIES and ESMACS, to demonstrate heterogeneous
protocol execution.
% Experiment 2 measures weak scaling performance of ESMACS at higher scales,
% using ORNL Titan.
\mtnote{Would it be better to describe weak scaling in terms of solution
time, number of resources and problem size, as done in the following
paragraph?}\jdnote{moved to previous para when first introducing weak
scaling}

We show homogeneous and heterogeneous weak scalability of the protocols
\mtnote{which protocols? So far we wrote about methods (but we did not
specify what type of methods are we referring to)} by increasing the number
of protocol instances while keeping the required \mtnote{required by
what/whom? ``keeping the number of pipelines fixed at the number required by
\ldots'' or something like that?} number of pipelines. By design of each
protocol, an increase in the number of instances means an increase in the
number of pipelines. Further,
\jdnote{mention the relationship between protocols and pipelines in section
IV.} by design of weak scaling, the ratio between the number of pipelines and
cores are kept constant. As the number of cores (measure of resource) changes
by a factor of 2, we introduce twice as many protocol instances. In this way,
weak scaling provides insight into the size of the workload that can be
investigated in a given amount of time.
% \mtnote{property of? I would replace with: ``weak scaling provides
% insight\ldots''} provides insight

Next, we measure strong scaling performance of the protocols \mtnote{which
ones?} by fixing the workload i.e. \mtnote{Grammar: please be careful about
the use of `i.e.' as explained in my previous set of comments} the number of
pipelines but reducing \mtnote{Why do we need the `i.e.' at all? What does
`workload' add? ``\ldots by fixing the number of pipelines of each protocol
and reducing \ldots''} the number of cores by a factor of 2 in each run. We
measure how the timing \mtnote{what does `timing' mean in this context?
Duration? } of the workload \jdnote{will be defined in section IV}\mtnote{we
will define `what'?} scales when adding resources, \jdnote{expecting linear
speedup, and expect fixed overheads}\mtnote{is this comment about text that
still needs to be written? If so, I would not mention expectations of results
in the design section.} as well as the overheads in \mtnote{of?} HTBAC, EnTK
and RP. Experiments 4 measures the strong scaling performance of ESMACS while
experiment 5 repeats the same strong scaling experiment using the TIES
protocol.

Lastly, we perform nonadaptive and adaptive experiments using the TIES
protocol to show intra-protocol adaptivity \jdnote{define intra-protocol
adaptivity in section IV}.\mtnote{This is a placeholder for an actual
paragraph: note how the previous paragraphs have a structure based on
numbering experiments (and referring them to the table via references that
are still missing), while this and the following paragraphs do not use the
same structure. Please expand as needed and note that a section is not
`ready' until this kind of uniformity is not achieved.}

% Keep iterating on the next two paragraphs to get the right description from
% Kristof
% -------------------------------------------------------------------------- %
In the non-adaptive workflow\mtnote{Reference figure}, we use 13 preassigned
and approximated $\lambda$ windows \mtnote{Reference table and exp number}.
By design of the TIES protocol, each $\lambda$ window creates a new
simulation condition that must be replicated. TIES requires 5 replicas per
$\lambda$, thereby producing $\lambda \times 5$ simulations. \jdnote{Section
IV will explain how simulations map to computational tasks}. As described in
the TIES workflow Figure \mtnote{Reference to the figure is missing}
\jdnote{include nonadaptive workflow}, this \mtnote{Grammar: it is not clear
to what this `this' refers to} produces a total of 65 concurrent simulations
for stages $S1$--$S4$ \mtnote{Reference table with S*} \mtnote{Grammar: no
spaces before and after dashes}. The production simulation stage $S4$
executes each simulation for 4ns. The analysis stage $S5$ assigns 5 analysis
tasks which aggregate simulation results of $S4$ with respect to each
replica. The global analysis stage $S6$ contains a single task that
aggregates the results from $S5$.

In the adaptive workflow~\ref{fig:adaptive_ties}, we run shorter 1ns MD
production simulations, followed by a different \mtnote{different from what?}
analysis which estimates free energy errors with respect to each interval of
two $\lambda$ values. The estimation of the error determines whether
\mtnote{Grammar: as already commented in the previous draft, `whether'
implies already `or not'. Adding it is redundant.} to spawn additional
$\lambda$ windows. If the error estimate in a certain interval is higher than
the acceptable threshold, a new $\lambda$ window is assigned halfway between
the interval. If the critical threshold is reached, we have reached
convergence and the application stops; else, current simulations continue
executing and new simulations begin executing based on newly generated
$\lambda$ windows.

The estimation computation \mtnote{This is too compressed. In itself,
`estimation computation' means little. The reader has to go back to the
previous paragraph to understand what those two words refer to. `We compute
the free energy errors\ldots'?} is captured \mtnote{`captured' is
metaphorical. Metaphors should be avoided when writing science. Also, the
sentence is passive and also this should be avoided in scientific writing} as
a single stage \mtnote{Does the reader know what a stage is?} containing a
single task. By design of the adaptive quadrature algorithm \mtnote{where
this algorithm is used? Does the reader know about it?}, the number of
additional $\lambda$ windows will vary across candidates, which will impact
the number of simulation-analysis iterations. \mtnote{And\ldots We are
missing an explanation of why this is important from an experimental design
point of view. Maybe because it changes the number of tasks?}
\jdnote{will call this something else}.\jdnote{check with Kristof on the
local and global analysis that happen for nonadaptive}

\mtnote{Note: paragraphs like this indicate lack of iteration before
releasing the manuscript for review. Too much is assumed, too little is
converted from thoughts to written form. How the academic writing process
works: Before releasing the manuscript for review, the author goes back and
forth across the manuscript multiple times. Progressively, each paragraph is
iterated until it is grammatically correct and it stands on its own
content-wise. When a manuscript is received for review, the reviewer focus on
the content of each paragraph and section, assessing their completeness and
conceptual/formal correctness. Each grammatical error, each inconsistency or
missing detail distracts the reviewer impeding her work.}

% This procedure is repeated until convergence, at which point all concurrent
% simulation are terminated. We define convergence as the point in the
% production-analysis loop at which a desired error threshold is reached.

% move this to HTBAC Design & Implementation
% -------------------------------------------------------------------------- %
% As discussed earlier, the free energy is calculated numerically using an
% integration algorithm, integrating the function $\partial
% U/\partial\lambda$ evaluated at discrete of $\lambda$. In a non-adaptive
% case these values of $\lambda$are selected a priori, and having no
% knowledge of the function's shape, equally spaced values are the optimal
% option.

% The aim of the general adaptive quadrature algorithm is to minimize the
% number of function evaluations while still estimating the integral under a
% specified error threshold. In our case a function evaluation means the full
% simulation for a given $\lambda$ window, and reducing this means a
% substantial efficiency gain.

% where each stage runs shorter production simulations of 1 nanosecond. We
% initialize $S4.1$ by assigning 3 equally spaced $\lambda$ windows at $0$,
% $0.5$ and $1$. Once the simulations have converged, we run a post-execution
% estimate the error of the free energy along each interval, i.e. $0$ to
% $0.5$ and $0.5$ and $1$. Once each simulation is complete within a stage, a
% decision is made about whether more $\lambda$ windows are required, and, if
% so, where these windows should be placed.

% We start the simulation with 3 equally spaces lambda windows at $0$, $0.5$
% and $1$ and run the three simulation in parallel just until they have
% converged. Then we estimate the error of the free energy along each
% interval (0 to 0.5 and 0.5 to 1) to determine whether a new lambda window
% needs to be placed halfway in between. The we iterate until no more lambda
% windows need to be placed. Now we determined the minimum set of optimally
% placed set of lambda windows. Using this set we start full simulation
% protocols to calculate the free energy of the alchemical transformation.

% -------------------------------------------------------------------------- %
% In the adaptive workflow, we alter the number of $\lambda$ windows being
% simulated over the course of a protocol instance. The position of new
% $\lambda$ windows depends on the estimated error of the integral measured
% between adjacent windows. Increasing the number of $\lambda$ windows in
% regions of rapid change will increase the accuracy of the overall integral
% to a greater degree than an arbitrarily placed window. In order to
% adaptively add lambda windows, we need access to the $\partial
% U/\partial\lambda$ values during runtime. Therefore, we break down the
% single production simulation stage (S4) \mtnote{I would reference the
% figure with the workflow diagram here} from the nonadaptive workflow into
% multiple smaller stages \mtnote{how many?}, each running for 1 ns. Once
% each simulation is complete within a stage, a decision is made about
% whether more $\lambda$ windows are required, and, if so, where these
% windows should be placed.

% We start out the simulation with 5 replicas of \emph{3} \mtnote{why
% emphasis for 3?} equally spaced $\lambda$ windows, and equilibrate them.
% Then we repeatedly execute shorter production simulations followed by an
% analysis phase which determines where to place new lambda windows. This
% procedure is repeated until convergence, at which point all concurrent
% simulation are terminated. We define convergence as the point in the
% production-analysis loop at which a desired error threshold is reached.

% In non-adaptive experiments we run simulations according to the original
% TIES protocol where the MD simulation step runs for the full 4 ns duration.
% In the adaptive experiments, we still adhere to the same TIES protocol,
% however we stop the MD simulation after each ns and allow the adaptive
% quadratures to calculate the SEM. The calculation determines where to place
% additional lambda windows in the next nanosecond duration. Once we reach a
% level of simulation convergence, we can halt simulations for that candidate
% and shift resources over to another candidate whose simulations have not
% yet started. We demonstrate computational efficiency, i.e. given X number
% of core hours, what is the difference between adaptive accuracy and
% nonadaptive accuracy? By extent we can also begin to inference the
% trade-offs between computational resources and accuracy.

% The benefit of running adaptive simulations produces greater fidelity.
% Since the exact placement of lambda windows and by extent the simulations
% are not known a priori, in the non-adaptive scenario, lambda windows are
% approximated, 13 evenly spaced positions. While this placement does produce
% statistical convergence, it does so at the cost of additional resources.
% Instead, in the adaptive scenario, we begin the MD simulation with 3
% approximated and evenly spaced lambda windows. We run the MD simulation for
% only 1 nanosecond, and aggregate the outputs from those lambda windows in
% order to calculate the RMSE which determines where to position additional
% lambda windows. Since this variability of lambda windows is highly
% dependent on the physical system, the only way to capture more accurate
% placement of lambda windows is during runtime in order to inference lambda
% windows based on intermediate results.

% We can glean two points from this:

% 1) We can stop the simulation once we reach an acceptable error threshold
% 2) To produce tighter binding affinity as some clinical insights may
%    require, given the same amount of core hours, what is the added benefit
%    of running adaptive experiments over nonadaptive, i.e. how much more
%    accuracy do we gain by running adaptive simulations?

% When one protocol has converged whilst another has not, we can shift
% computational resources to favor simulations that require additional
% resources.

% individual\mtnote{I am not sure I understand what individual means in this
% context}

% strong scaling performance of ESMACS and TIES on Blue Waters. We fix the
% number of protocols and vary the amount of resources in order to produce
% generations

% \mtnote{The reader will struggle to
% understand what `generations' means in this context.} of execution.
% \mtnote{Two sizable paragraphs to describe weak scaling, four lines to
% describe strong scaling? I would expand.}

Due to the NCSA security policies, we performed the experiments from a
persistent virtual machine \mtnote{This sentence seems to suggest that we run
our experiments on a virtual machine. I know what you mean but the average
reader has no idea about the capabilities of our middleware and the policies
of not executing persistent processes on the head node of Blue Waters.}
% and the Titan ones from an ORNL login node.
% as Blue Waters does not permit executing applications directly on the login
% node. The ORNL Titan experiments had instead to be performed from an ORNL
% login node.
We used HTBAC 0.1, EnTK 0.6, and RP 0.47 for all the experiments and the same
versions of the NAMD MD engine, compiled according to the capabilities of
each environment and of NAMD itself. For all experiments we used
\texttt{NAMD-MPI}. Additionally, for the analysis stages in the TIES protocol
we used \texttt{AmberTools}.

For weak and strong scaling experiments \mtnote{reference table}, we
assigned the minimization tasks of $S1$ to 1000 simulation time steps, while
the equilibration tasks in $S2$ and $S3$ are \mtnote{Grammar: inconsistent
tense. `Were' or `assign'} assigned 5000. The MD simulation tasks in $S4$ are
assigned 50,000 steps. \jdnote{the reader will be able to understand $S$ once
I fix section IV} \jdnote{Kristof to include the number of timesteps for
adaptive/nonadaptive experiments}

% \mtnote{when/why a simulation task is `production'? I am not sure I
% understand what that means}\jdnote{removed production for weak/strong.
% Production applies to adaptive/nonadaptive which need to show the
% production scale timesteps i.e. 2 or 3M}

% while on Titan a non-MPI, multi-core NAMD engine. On Titan we compiled NAMD
% with CUDA for the ESMACS experiments, and with OpenMP for the for TIES ones
% as NAMD does not support alchemical perturbations execution on GPU.

We perform \mtnote{Grammar: We used the preterite until now} all Blue Waters
experiments using the \texttt{aprun} application launch method, which is the
NCSA Blue Waters designated command for describing the application parameters
and resource requirements \mtnote{does the reader know what is a launch
method?} \jdnote{better?} \mtnote{Unfortunately not:
https://bluewaters.ncsa.illinois.edu/using-aprun . Alternative: `We perform
(or performed depending on what tense you will decide to use) all Blue Waters
experiments using the \texttt{aprun} command to launch the MD simulations'}.
Unfortunately, \texttt{aprun} has several \mtnote{this is too generic. Based
on this paragraph, we found two main limitations. This is what we should
therefore write.} limitations: First, the number of
concurrent tasks execution is limited to \ldots. On average, during our experiments we were not able to execute more than 436 concurrent tasks. \mtnote{how do we know
this average?}.
\jdnote{would it make sense to show a bell curve of the distribution of
failing units? I have 8 data points, but I can run more and include Mark's
Cray paper?}\mtnote{Agreed. The comment was about saying that you observed
this average while performing experiments. See how I rewrote the paragraph}.
Second, \texttt{aprun} does not permit multiple \texttt{MPI} applications to
land \mtnote{this is metaphorical. Please change using a factual predicate.}
on the same node. As a consequence, we had to run each MD simulation with 32
cores (i.e., one full compute node) even if our performance measurement of
NAMD on Blue Waters indicated that 16 cores would offer the best trade-off
between computing time and communication overhead.\mtnote{Note: I had to
rewrite the paragraph because the current version was too unrefined. Please
see the note above about the need to progressively refine each paragraph.}

% From our own scalability performance measurements of NAMD on Blue Waters,
% we observe the ideal cores per task to be 16, however Blue Waters does not
% permit running multiple MPI applications on the same node, hence each NAMD
% task requires a complete node to maintain concurrency.
% \mtnote{This needs to go before previous sentence as it explains why we use
% 32 cores for each NAMD executable. Note: it is not true that NAMD
% `requires' 32 cores as stated above.}\jdnote{moved here since this para
% discusses \texttt{aprun} limitations}

% In order to continue execution on higher scales \mtnote{Why do we need an
% higher scale?}, we perform higher scales \mtnote{I am not sure what
% `performing higher scales' means} on ORNL Titan, using the ORTE launch
% method \mtnote{Why don't we use ORTE on BW? The reader will probably know
% that they are similar machines}.

% The difference in platforms produces overheads \mtnote{Only overheads or
% also different execution time of NAMD---seeing the differences described in
% the previous paragraph} that can be captured by RTS \mtnote{does the reader
% know what a RTS is?} and are shown \mtnote{careful with the use of
% `demonstrate', especially in a scientific paper} in the figures
% \mtnote{which figures?} as "APRUN overhead" and "ORTE overhead"
% \mtnote{LaTeX requires different quotes}.

% For tasks pertaining to $S1$ -- $S4$ \mtnote{What are $S1$ and $S4$? LaTeX
% wants double dash without space to indicate a range}, while the analysis
% stage, $S5$ use AmberTools \mtnote{grammar: I am not able to parse this
% sentence}.

% For both adaptive and nonadaptive experiments \mtnote{the reader knows
% about Experiment 1-7, not about adaptive/nonadaptive ones},

% In the nonadaptive experiments,

% For the adaptive experiments, each substage
% \mtnote{What is a `substage'?} of $S4$ i.e. $S4.1$--$S4.4$ is assigned
% 50,000 steps.\mtnote{Why the difference is number of steps and why it is
% relevant?}

% HTBAC submits a resource request to EnTK, to which EnTK uses RP to acquire
% resources via a single pilot. Accordingly, we request the maximum number of
% cores required by the workload as the number of cores in a pilot.

% To characterize the weak scaling performance of TIES \mtnote{I am not sure
% I understand this sentence: For measuring the weak scaling performance of
% TIES?}
% \jdnote{better?}, we use between 4160 and 33280 cores as indicated in
% Figure~\ref{fig:weak_scaling_TIES} because the NAMD executable used in all
% tasks \mtnote{does the reader know what a task is in this context?}
% \jdnote{will be defined in Section IV} from $S1$-$S4$ require
% \mtnote{grammar: requires} a single node i.e. 32 cores per task, as
% mentioned earlier in the APRUN limitation.
% \mtnote{why?}\jdnote{32 cores/task addressed by earlier mention of aprun
% MPI limit}

% For strong scaling performance, we fix the number of protocol instances to
% 8 instances given that we experience failing concurrency of tasks when we
% scale the workload to 8 protocols.\mtnote{why 8?} We vary the amount of
% resources as shown in Figure~\ref{fig:strong_scaling_TIES} by showing
% reducing of resources by a factor of 2. \mtnote{Please expand indicating
% and justifying the the chosen amount of resources. Experiments can be
% design in many ways, we need not only to describe but also to justify why
% we choose a specific design for our experiments.}\jdnote{better?}

% For the TIES protocol, each pipeline \mtnote{does the reader know what a
% pipeline is?}\jdnote{will be address in Section IV} consists of six stages
% \mtnote{simulation stages?}. Each of the simulation stages contains a task
% for every unique ($\lambda$, replica) combination \mtnote{does the reader
% understand this?}\jdnote{will be addressed better in section IV}. In the
% non-adaptive workflow \mtnote{we never used `workflow'. Previously we used
% non-adaptive (written as `nonadaptive') experiment. Note that we did not
% introduce adaptive/nonadaptive experiments} scenario, the first 11
% $\lambda$ windows
% \mtnote{does the reader know what a lambda window is?} consist of the
% following values: $L$ is a vector with
% \begin{flalign} L &= \{ x_i: x_i\in[0,1]\; and\; x_{i+1} = x_i + \delta \},
% where\ \delta\ is\ 0.1. %&$$L=\{ x_i: x_i\in[0,1]\; and\; x_{i+1} = x_i +
% \delta \}$$%, where $\delta$ is $0.1$.
% \end{flalign}

% We append two additional values on both ends of $L$, completing 13
% $\lambda$ windows. Each $\lambda$ window consists of five replicas.
% Therefore there are a total of 65 tasks for every simulation stage
% \mtnote{depending on the previous sections of the paper, this `therefore'
% may have to be better explained to the reader}. The production simulations
% stage, $S4$ as described in figure~\ref{fig:pst}\mtnote{missing figure}
% executes a 4 ns simulation duration \mtnote{grammar: executes a simulation
% for 4 ns? a simulation with a 4 ns duration?}. The analysis stages of the
% protocol reduce the number of tasks \mtnote{Why/how?}. The first analysis
% task consists of five tasks where each task performs an aggregate analysis
% over all $\lambda$ windows for each replica. The second analysis stage
% consists of one task that aggregates the previous results and computes a
% single average across all replicas.

%----------------------------------------------------------------------------
\subsection{Weak Scaling and Performance Characterization}

% \mtnote{Is this meant to be a subsubsection?}\jdnote{changed weak, strong
% and adaptive experiments \& results to subsections}

We investigate the weak scaling of HTBAC using three
applications\mtnote{which ones?}: \ldots, \ldots and \ldots. Each application
uses a different protocol configuration with a number of instances determined
by the number of concurrent executions supported by \texttt{aprun}. In each
experiment, we increase the number of instances of the TIES protocol,
proportionally to the amount of requested resources. 

Experiment 1 measures the weak scalability of HTBAC with the TIES protocol.
Each instance of the TIES protocol contains a single pipeline with 4 stages
and 65 concurrent tasks. We increase the number of instances linearly,
between 2 and 8. Experiment 2 measures weak scalability with the ESMACS
protocol. Each ESMACS protocol contains 1 pipeline with 4 stages and 25
concurrent tasks \mtnote{Do we `increase the number of instances linearly,
between 2 and 8' also in experiment 2? If so, we should say that only once
for Experiment 1--3. If not, why?}. Experiment 3 measures weak scalability
with instances of both TIES and ESMACS protocols. Also in this case, we scale
the instances of both protocols linearly, between 2 and 8. For all weak
scaling experiments we used \mtnote{Grammar: conjugation} physical systems
from the \texttt{BRD4-GSK} library with the same number of atoms and similar
chemical properties. \jdnote{this is to say that the fluctuations in
execution times between data points are invariant to the physical
systems}\mtnote{Nice comment, I would edit it and put it in the text!}.

% \mtnote{does the reader know what a task is in this context?} \jdnote{will
% be defined in Section IV}

We characterize the execution time of the application \mtnote{which one?},
and overheads of HTBAC, EnTK, RP and \texttt{aprun}.  The total time to
completion (TTC) of each application \mtnote{of what?} \jdnote{per
application} \mtnote{you see the difference produced by my editing?} can be
expressed as $TTC = TTX + T_{O}$ where \(TTX\) measures the aggregated
duration of all task of an application.
\mtnote{TTX should measure only the aggregated time spent executing the
tasks' executables. All the rest should be part of RP overhead}\jdnote{for
discussion with Matteo 06/04}\mtnote{I am afraid I do not understand this
comment} $T_{O}$ equates to the total overheads produced by HTBAC, EnTK, and
RP: $$T_{O} = T_{O}\textsuperscript{HTBAC} + T_{O}\textsuperscript{EnTK} +
T_{O}\textsuperscript{RP}$$ \mtnote{As defined, this formula seems to add
twice part of the RP overheads.}\jdnote{I think I'm missing something: The RP
overhead is what I get from the pilot duration logs. Not sure where I'm
duplicating}\mtnote{If TTX is more than just the aggregated execution time of
all the tasks' executable, then TTX and $T_{O}$ both contain part of RP
overhead.}

% \mtnote{grammar: I do not understand the sentence after the `;'} given by
% the sum of the constituent overheads: $$T_{O} =
% T_{O}\textsuperscript{HTBAC} + T_{O}\textsuperscript{EnTK} +
% T_{O}\textsuperscript{RP}$$.

In all weak scaling experiments (Figures~\ref{fig:weak_scaling_TIES},
~\ref{fig:weak_scaling_ESMACS} and~\ref{fig:weak_scaling_ESMACS_TIES}) we
observe TTX (right axis) to be within error margin and showing ideal weak
scaling behavior \mtnote{Please refine the previous sentence as it might not
say what I think you want to say: What does it mean that TTX is `within error
margin'? And how this relates to `ideal weak scaling behavior'? Maybe the
former applies to the latter instead of TTX?}. The error bars are calculated
using 3 trials per protocol instance measure.\mtnote{This might be consider
little without further explanation}

The HTBAC overhead depends mostly on the number of protocol instances that
need to be generated for an application. This overhead shows a super linear
increase as we grow the number of protocol instances, but its duration is
negligible when compared to \(TTX\).
% HTBAC enables concurrent execution of multiple protocol \mtnote{what type
% of protocols?} instances.

\mtnote{I would create a subsection with overheads, discussing their behavior
and nature while referencing both weak and strong scaling experiments.}

The runtime overheads, mainly EnTK and RP, mostly depend on the number of
tasks that need to be translated in-memory from a Python object to a compute
unit (CU) description~\cite{dakka2017}. \mtnote{This sentence is copied from
a previous paper we wrote: does the reader know what a CU is?}
\jdnote{I will add in a brief mention of CU in Section IV}\mtnote{I would
just use task as you do in the following sentence. IMHO, using CU adds yet
another pseudo-technical term as a synonym of a term we are already using,
i.e., task}. As such, those overheads are expected to grow proportionally to
the number of tasks as observed in Figure\ldots, bar in \ldots color, \ldots.\mtnote{Please add reference to the relevant figure(s) and details.} 

\mtnote{I commented all the following. IMHO, it does not contribute to
understand the current analysis of the results. Do we need to expand the
analysis?}
% ------------------------- COMMENTED BY MT -------------------------
% EnTK submits CU descriptions to a database that is then pulled by
% RP.\mtnote{I am not sure this is accurate}\jdnote{took out "same" database}
% In addition, each stage constructed by EnTK maintains sequential barriers.
% \mtnote{Doe the reader know what a barrier is?}\jdnote{I will add this to
% the EnTK description in Section IV} RP remains dormant until completion of
% the current tasks before staging the next tasks. \mtnote{This figure is
% about scaling behavior, not about the mechanics of EnTK and RP here
% discussed.}\jdnote{took out figure ref}
% ------------------------- END COMMENTED BY MT ----------------------

% The impact of the synchronization barriers increases with the number of CUs
% as seen in the 16 protocol instances data point in
% This pull operation occurs over a wide area networks, which introduces
% varying amounts of latency.

The RP overhead is calculated measuring and aggregating the execution time
(i.e., duration) of the RP components that manage and coordinate the
execution of the application. Among these components, the task scheduler of
RP introduces the largest overhead. The task scheduling overhead scales
linearly with the number of tasks because the time taken to schedule a task
depends on the total number of tasks that need to be scheduled. 
% The main contributor to the increase in overhead in RP is based on the the
% time of resources inactivity while RP schedules new tasks. As such, the
% overhead is expected to grow proportionally to the number of concurrent
% tasks~\cite{dakka2017}.
\mtnote{I am afraid this whole paragraph needed to be
rewritten. I tried to fix it but we are missing a comparative evaluation of
this overhead. For example, does this overhead matter when compared to TTX or
TTC? Given a computation campaign with TIES and ESMACS, how much allocation
time would be spent on this overhead? What would be the total percentage of
allocation/resources spent in overheads?}

% Among these durations, the pilot duration contains the time to bootstrap
% and terminate the pilot. The task overheads contain the executor's spawning
% of the task, folder staging preparation, and operating system task
% spawning. \mtnote{We need to explain to the reader why all this is relevant
% in this context.} \jdnote{need to discuss on 06/04, should we keep these
% details or keep the focus on the paragraph just above this one?}

Furthermore, an additional overhead, driven by the \texttt{aprun} launch
method increases as we approach a system limit on the number of concurrent
\texttt{aprun} processes. For example, when scaling to 8 TIES protocol
instances as shown in Figure~\ref{fig:weak_scaling_TIES}, the application
requires 520 concurrent tasks. Considering that we observe \texttt{aprun}
failures at approximately 436 concurrent processes, this accounts for
additional time that is factored into \(TTX\). \jdnote{add in the aprun
graph} \mtnote{We need to explain why TTX is affected. Are tasks restarted?
Also, does this overhead matter? See previous comment about RP overhead
impact on TTX/TTC/Resource utilization.}

% \mtnote{this sentence does not read well. I would refine it}
% \mtnote{this needs further explanation}
% \jdnote{reworded paragraph, also commented out next paragraph on EnTK
% synchronization until I can justify better.}

% \mtnote{Does the reader know what aprun is? Also, I would use something
% like textttt to identify the name of a command or
% executable}\jdnote{addressed}

% \mtnote{concurrent?}\jdnote{addressed} 

% Together, the EnTK-enabled synchronization barriers and \texttt{aprun}
% overhead failures \mtnote{until now, we have implicitly assumed that our
% overheads are measured in time. Did we now use also number of failures?}
% introduce delays in the scheduling of the CUs and results in higher
% overheads \mtnote{Is this sentence circular? Overhead increases
% contributing to increasing the overhead?}. Lastly, we notice that each
% additional protocol instance contributes to roughly 55 additional seconds
% in \(TTX\).\mtnote{All this needs to be clarified and expanded into an
% actual discussion of the results.}

% We use between 4160 and 16,640 cores as indicated in
% Figure~\ref{fig:weak_scaling_TIES}.

% To characterize the weak scaling performance of TIES \mtnote{I am not sure
% I understand this sentence: For measuring the weak scaling performance of
% TIES?}
% \jdnote{better?}, we use between 4160 and 33280 cores as indicated in
% Figure~\ref{fig:weak_scaling_TIES} because the NAMD executable used in all
% tasks \mtnote{does the reader know what a task is in this context?}
% \jdnote{will be defined in Section IV} from $S1$ -- $S4$ require
% \mtnote{grammar: requires} a single node i.e. 32 cores per task, as
% mentioned earlier in the APRUN limitation.
% \mtnote{why?}\jdnote{32 cores/task addressed by earlier mention of aprun
% MPI limit}

% With each new protocol instance generated for an application, the HTBAC
% overhead grows to match the additional requirement of generating new
% protocols.

% In order to understand the contribution of the various events in HTBAC,
% termed as HTBAC overhead, to

%----------------------------------------------------------------------------
% \subsubsection{Weak Scaling Experiments}

% We investigated the weak scalability properties for the TIES protocol by
% growing the number of protocol instances while adhering to the required
% number of pipelines. By design of each protocol, an increase in the number
% of instances simply means an increase in the number of pipelines. The first
% weak scalability experiment demonstrates the behavior of HTBAC, EnTK and RP
% using the multiple instances of the TIES protocol. By design of weak
% scaling, the ratio between the number of pipelines and cores are kept
% constant. As the number of cores (measure of resource) changes by a factor
% of 2, we introduce twice as many protocol instances. As designed, the weak
% scaling property provides insight into the size of the workload that can be
% investigated in a given amount of time.\mtnote{This paragraph is a copy of
% a previous paragraph. See comments and iterations of the previous paragraph
% before rewriting a new paragraph if needed.}

\begin{figure}
  \centering
    \includegraphics[width=\columnwidth]{figures/new_ws_ties.pdf}
    \caption{Weak scaling properties of HTBAC using TIES protocol. We
    investigate the weak scaling of HTBAC as the ratio of the number of
    protocol instances to resources is kept constant. Overheads of HTBAC
    (right), and runtime overhead (left) and \(TTX\) (left) for experimental
    configurations investigating the weak scaling of TIES. We ran 3 trials
    for each protocol instance configuration.}
\label{fig:weak_scaling_TIES}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=\columnwidth]{figures/new_ws_esmacs.pdf}
    \caption{Weak scaling properties of HTBAC using ESMACS. We investigate
    the weak scaling of HTBAC as the ratio of the number of protocol
    instances to resources is kept constant. Overheads of HTBAC (right), and
    runtime overhead (left) and \(TTX\) (left) for experimental
    configurations investigating the weak scaling of TIES. We ran 2 trials
    for each protocol instance configuration.}
\label{fig:weak_scaling_ESMACS}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=\columnwidth]{figures/new_ws_esmacs_ties.pdf}
    \caption{Weak scaling properties of HTBAC using ESMACS and TIES. We
    investigate the weak scaling of HTBAC as the ratio of the number of
    protocol instances to resources is kept constant. Overheads of HTBAC
    (right), and runtime overhead (left) and \(TTX\) (left) for experimental
    configurations investigating the weak scaling of TIES. We ran 2 trials
    for each protocol instance configuration.}
\label{fig:weak_scaling_ESMACS_TIES}
\end{figure}

\jdnote{Maybe combine captions into 1?}


% ---------------------------------------------------------------------------
\subsubsection{Strong Scaling and Performance Characterization}

\mtnote{Should we have also a `Weak Scaling Experiments' header? Or maybe we
should change this one?}

To investigate strong scaling, we run two applications on Blue Waters: \ldots
and \ldots \mtnote{Please add name of the applications}. We investigate the
strong scaling behavior considering only homogeneous protocols. The
investigation of heterogeneous protocols is of interest but not at the risk
of cluttering the paper \mtnote{I am afraid we will need a better explanation
for this omission}. Both applications use the \texttt{BRD4-GSK} physical
systems \mtnote{Should this detail go in experiment design? This would apply
also to the weak scaling in the previous section}.

The first application \mtnote{name?} shows the strong scaling behavior of
TIES alone \mtnote{Are you sure this should not be the `nth experiment shows'
instead of the `first application shows'?}, using \texttt{NAMD-MPI} with 32
cores per task. We fix the number of instances of the TIES protocol to 8 due
to the described \texttt{aprun} limitations. We vary the amount of resources
between 4160, 8320 and 16640 cores. Given 4160 cores, we can execute 4
generations of 130 concurrent tasks; with 8320 cores, we can execute 2
generations of 260 concurrent tasks. The second application shows the strong
scaling behavior of the ESMACS protocol \mtnote{See previous comment about
application/experiment}. We fix the number of instances of the ESMACS
protocol to 16 and vary the amount of resources between 3200, 6400 and 12800
cores. This produces the same number of generations of execution \mtnote{add
reference to the subsection where generation is defined} as the first
application. \jdnote{will add the plot for ESMACS ss once I receive 2 full
trials}. As we increase the number of parallel resources,
Figure~\ref{fig:strong_scaling_TIES} shows linear speedup in \(TTX\) while
maintaining fixed $T_{O}$. The fixed $T_{O}$ suggests that the overheads are
mainly driven by the scheduling in the number of protocols and thereby the
number of tasks, not by the amount of resources. This is confirmed for RP in
Ref.~\cite{merzky2018}.\jdnote{I need to investigate this but I also presume
that aprun limits are also a factor of overhead.}

% The comparison between weak and strong scalability shows the overhead
% introduced by load balancing and scheduling tasks in multiple generations.

% By design of the TIES protocol, this workload show up to $65 \times 8$
% concurrent tasks, where each task uses 32 cores. The total number of
% concurrent

% \mtnote{See previous comments about terminology}\mtnote{I am not sure I
% understand: are we comparing overheads when weak and strong scaling our
% applications/workflows or are we first measuring the overheads when strong
% scaling and then comparing them to those of weak scaling?}. As we scale the
% number of generation of concurrent tasks executions, we half the resources
% allocated by the pilot \mtnote{this should be explained in the experiment
% setup and design}.

\mtnote{General note. Experiments about weak and strong scaling show the
behavior of our software stack with one or more workflows/workloads. The
question these experiments answer to is: Given a workload/workflow, does this
software stack scale weakly and/or strongly? This is why, in the analysis of
our data, we look at the linearity (or lack of thereof) of our plots. The
analysis of the overhead(s) answer to a different question: What part of a
measure---e.g., time to completion---depends on the properties of an element
used to produce that measure---e.g., a component of our software stack?
Usually, the analysis of the overheads is used to explain why we observe lack
of scalability (weak or strong) as represented by a superlinearity in our
plots. The discussion of our experimental results should clearly distinguish
these questions and properly relate the study of scalability to that of
overheads. At the moment, our discussion does not do all this.}\jdnote{I
reduced the discussion of scalability down to the properties that relate to
contributions of overheads and trends observed in TTX.}

\begin{figure}
  \centering
   \includegraphics[width=\columnwidth]{figures/new_ss_ties.pdf}
    \caption{Strong scaling properties of HTBAC using TIES protocol. We
    investigate the strong scaling of HTBAC with a fixed number of protocol
    instances while varying the amount resources. Overheads of HTBAC and
    runtime overhead (left) and \(TTX\) (right) for experimental
    configurations investigating the strong scaling of TIES. We ran 3 trials
    for each configuration.}
\label{fig:strong_scaling_TIES}
\end{figure}



%----------------------------------------------------------------------------
\subsection{Experiments Validation}

% HTBAC automates the process of calculating the binding affinity of
% protein-ligand complexes from reading the input to analyzing the final
% results.

In order to validate the correctness of the results produced using HTBAC and
the BRD4-GSK physical systems, we compare our results with those previously
cited \mtnote{This is unclear. Do you mean: results that have been previously
published? If so, where?}. These experiments \mtnote{What experiments? The
previous sentence writes about a comparison.} are vital to gain confidence in
the algorithm \mtnote{which algorithm?} and to prove that it is indeed
calculating the correct values.

The validation experiments were based on the original study of Wan et.
al.~\cite{Wan2017brd4}. We selected a subset of the protein ligand systems
that were the subject of that study: they are the ligand transformations 3 to
1, 4, and 7. We then performed a full simulation on all 3 systems and
calculated the binding affinity (see Table~\ref{tab:exp2}) using HTBAC.

The results show that all three $\Delta \Delta$G values are within error bars
of the original study using the TIES protocol, reinforcing the fact that our
results produced using HTBAC are indeed correct.

\begin{table}
  \centering
  \caption{Comparison of the calculated binding free energies using HTBAC,
  from the original study by Wan et. al and experimental data. In principle,
  the two theoretical studies used the same protocol. This experiment proved
  that HTBAC implemented TIES correctly, as the calculated values are either
  the same or within error bar of the original study. All values are in
  \textbf{kcal mol\textsuperscript{-1}}.}  % \begin{tabular}{l@{\hskip 1in}r@{\hskip 0.2in}r@{\hskip 0.2in}r}
  \begin{tabular}{lrrr}
    \toprule
    System & HTBAC & Wan et. al & Experiment \\
    \midrule
    BRD4 \textbf{3 to 1} & \num{0.39 +- 0.10} &   \num{0.41 +- 0.04} &  \num{0.3 +- 0.09} \\
    BRD4 \textbf{3 to 4} & \num{0.02 +- 0.12} &   \num{0.01 +- 0.06} &  \num{0.0 +- 0.13} \\
    BRD4 \textbf{3 to 7} & \num{-0.88 +- 0.17} &  \num{-0.90 +- 0.08} & \num{-1.3 +- 0.11} \\
    \bottomrule
  \end{tabular}
  \label{tab:exp2}
\end{table}

%----------------------------------------------------------------------------
% \subsection{Adaptive Experiments}

% The TIES workflow can benefit from an adaptive execution environment to
% improve the efficiency and accuracy of result. In \emph{adaptive
% experiments} we implemented the adaptive quadrature algorithm specifically
% customized for biosimulations.

% In the adaptive workflow, we alter the number of $\lambda$ windows being
% simulated over the course of a protocol instance. The position of new
% $\lambda$ windows depends on the estimated error of the integral measured
% between adjacent windows. Increasing the number of $\lambda$ windows in
% regions of rapid change will increase the accuracy of the overall integral
% to a greater degree than an arbitrarily placed window. In order to
% adaptively add lambda windows, we need access to the $\partial
% U/\partial\lambda$ values during runtime. Therefore, we break down the
% single production simulation stage (S4) \mtnote{I would reference the
% figure with the workflow diagram here} from the nonadaptive workflow into
% multiple smaller stages \mtnote{how many?}, each running for 1 ns. Once
% each simulation is complete within a stage, a decision is made about
% whether more $\lambda$ windows are required, and, if so, where these
% windows should be placed.

% We start out the simulation with 5 replicas of 3 \mtnote{why emphasis for
% 3?}\jdnote{addressed} equally spaced $\lambda$ windows, and equilibrate
% them. Then we repeatedly execute shorter production simulations followed by
% an analysis phase which determines where to place new lambda windows. This
% procedure is repeated until convergence, at which point all concurrent
% simulation are terminated. We define convergence as the point in the
% production-analysis loop at which a desired error threshold is reached.

% The success of this algorithm is determined by the decision where
% additional $\lambda$ windows should be introduced. In adaptive quadrature,
% this decision is made by calculating an error estimate on the integral and
% comparing this estimate to a threshold value. Due to the stochastic nature
% of biosimulations, it is non-trivial to determine this error and, as a
% proof of concept, we simplified this decision to replicate pre-calculated
% results. In future studies, we plan to use a dynamic decision process.

% Inter-node communication introduces a constraint on the number of new
% $\lambda$ windows that can be added at each iteration. Simulations must run
% on an integer number of nodes to reduce the overhead of inter-node
% communication. This means that the number of new $\lambda$ windows (i.e.,
% the number of simulations) \emph{has} to be either doubled or left
% unchanged. If the number of windows is doubled, the number of nodes per
% simulation can be halved automatically. Our algorithm loops through the
% current $\lambda$ window pairs until this criterion is reached, forcefully
% adding more windows when needed.


% I don't think we need this equation here, it's too trivial.
% \begin{flalign} L &= \{ x_i: x_i\in[0,1]\; and\; x_{i+1} = x_i + \delta \},
% where\ \delta\ is\ 0.5. %&$$L=\{ x_i: x_i\in[0,1]\; and\; x_{i+1} = x_i +
% \delta \}$$%, where $\delta$ is $0.1$.
% \end{flalign}

% For every $\lambda$ window we initialize with five replicas therefore
% yielding a total of 15 tasks. We run 15 tasks for stages $S1$ through
% $S4.1$. Between stages $S4.1$ and $S4.3$ the number of $\lambda$ windows
% doubles for every stage, which doubles the total number of tasks. The last
% production simulation stage, $S4.4$, runs for the remaining 2 ns durations.

% Our experiments implement adaptive change in the $\lambda$ windows sampled
% and not the timing of execution. In this way, we introduce only a single
% \mtnote{additional?} degree of freedom relative to \mtnote{compared to?}
% our baseline ``non-adaptive" experiments. HTBAC provides the functional
% capability to adaptively determine the time at which the $\lambda$ windows
% are chosen. However, in this paper we do not investigate the impact of such
% adaptivity, as the objective is to determine the feasibility of adaptive
% execution and the scientific merit of adaptive decision making.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figures/Adaptive_TIES_1.png}
  \caption{\mtnote{For quality reasons, it would be better to use 
  figures in pdf format.}Illustrating the adaptive workflow. After the 3
  initial lambda windows are equilibrated, the first production stage starts.
  This is followed by analysis at every lambda interval, to decide whether to
  add a new window in the middle. The production-analysis is repeated for 4
  production steps in our implementation, not shown here.}
\label{fig:adaptive_TIES}
\end{figure}

%----------------------------------------------------------------------------


% \begin{figure}
%   \centering
%     \includegraphics[width=\columnwidth]{figures/adaptive_vs_nonadaptive_ps
%     eudo.pdf}
%     \caption{Illustrating the adaptive vs. non-adaptive workflow given the
%     same number of core hours}
% \label{fig:adaptive_vs_nonadaptive_TIES}
% \end{figure}

% ---------------------------------------------------------------------------
\subsection{Adaptive Experiments}

\mtnote{Should this be Results as a whole? Or are we missing the results for
all the others?} \kfnote{I think results and experiments are one section.
This subsubsection is for the adaptive quadrature. There is a separate one
for adaptive termination.}

The aim of introducing adaptive quadrature to alchemical free energy
calculation protocols (e.g. TIES) is to reduce time to completion while
maintaining (or increasing) the accuracy of the results. Time to completion
is measured by the number of core hours consumed by the simulations. Accuracy
is defined as the error with respect to a reference value, calculated via a
dense $\lambda$ window spacing (65 windows). This reference value is used to
establish the accuracy of the non-adaptive protocol (which has 13 $\lambda$
windows) and the adaptive protocol (which has a variable number of $\lambda$
windows, determined at run time).

One of the input parameters of the adaptive quadrature algorithm is the
desired acceptable error threshold of the estimated integral. We set this
threshold to the error of the non-adaptive algorithm calculated via the
reference value. The algorithm then tries to minimize the number of $\lambda$
windows constrained by the accuracy requirement. Table~\ref{tab:adapquad}
shows the results of running adaptive quadrature on 5 protein ligand systems,
comparing the \(TTX\) and accuracy versus the non-adaptive case. The number
of lambda windows are reduced on average by \SI{32}{\percent}, hence reducing
\(TTX\) by the same amount. The error on the adaptive results is also
decreased, on average by \SI{77}{\percent}. In the case of the TYK2 L7-L8
system, the non-adaptive error was very low, hence the algorithm implemented
a larger number of windows in comparison to the non-adaptive in match that
accuracy. In fact, by using 1 more window it was able to increase accuracy by
\SI{40}{\percent}.

\begin{table*}
  \caption{Comparing results of adaptive, non-adaptive and reference runs}
  \label{tab:adapquad}
  \begin{tabular}{lSSSSSS}
    \toprule
    {System}                               & 
    {Ref $\Delta \Delta$G}                 &
    {Non-adaptive $\Delta \Delta$G}        &
    {Adaptive $\Delta \Delta$G}            &
    {Num. of $\lambda$ windows}            &
    {Decrease in time to completion}       &
    {Increase in accuracy}                 \\
    \midrule
    {PTP1B L1-L2}   & 
    -58.51 & 
    -57.87(64) & 
    -58.60(9) & 
    10 & 
    \SI{23}{\percent} & 
    \SI{86}{\percent} \\
    %
    {PTP1B L10-L12} & 
    1.83   & 
    2.05(22) & 
    1.94(7)  & 
    6  & 
    \SI{54}{\percent} &
    \SI{68}{\percent} \\
    %
    {MCL1  L32-L38} & 
    2.13   & 
    2.33(20) & 
    2.14(1)      & 
    7  & 
    \SI{46}{\percent} & 
    \SI{95}{\percent} \\
    %
    {TYK2  L4-L9}   &
    -28.69 & 
    -28.25(44) & 
    -28.67(1)  & 
    7  & 
    \SI{46}{\percent} & 
    \SI{98}{\percent} \\
    %
    {TYK2  L7-L8}   & 
    4.97   & 
    4.92(5) & 
    5.00(3)      & 
    14 &  
    \SI{-8}{\percent} & 
    \SI{40}{\percent} \\
    \bottomrule 
    
  \end{tabular}
\end{table*}

Figure~\ref{fig:adapquad} shows progression of the algorithm, starting from 3
initial lambda windows, and at each iteration bisecting the interval where
the error is above the set threshold. For system X, we can see as how the
parts of the function that change less have relatively fewer windows, while
parts of greater change (usually, though not always around the $\lambda=0.5$
mark) have more $\lambda$ windows to represent the shape of the function more
closely.

\kfnote{Move this to algorithm description} In previous studies \cite{} the
error estimation during the adaptive algorithm is done by calculating the
integral estimate using two methods, one lower and one higher in complexity,
then the difference between the two can constitute as a rough estimate of the
error on that interval.

One way to visualize the adaptivity in this experiment is to plot the
function and integral approximations at every iteration.
Figure~\ref{fig:adapt} shows how the value of $\Delta$G (i.e., the integral
of the function) for test system \textbf{3 to 7} \mtnote{I would not mix
different formats of emphasis: should we use only emph?} improves as the
algorithm places more lambda windows. It is important to note that the new
points are decided {\it a priori} for reasons discussed above but this is
opaque to the algorithm. This shows we have initial adaptive capabilities,
paving the way to more advanced biosimulation algorithms.

\begin{figure}
  \centering
\begin{tikzpicture}
\begin{axis}[
  xlabel=$\lambda$,
  ylabel=$\frac{dU}{d\lambda}$,
  xmin=0,
  xmax=1,
  legend pos=north west,
  grid=both,
  ]
  \addplot+[name path=alch_1, mark size=1pt, mark=*, color=blue] table [x=lambda, y=p1v]{alch_1.csv};
  \addlegendentry{Iteration 1: Initial};

  \addplot+[name path=alch_2, mark size=1pt, mark=*, color=red] table [x=lambda, y=p1v]{alch_2.csv};
  \addlegendentry{Iteration 2: Increase};

  \addplot+[name path=alch_3, mark size=1pt, mark=*, color=brown] table [x=lambda, y=p1v]{alch_3.csv};
  \addlegendentry{Iteration 3: Optimized};

  \addplot+[name path=alch_4, mark size=1pt, mark=*, color=black] table [x=lambda, y=p1v]{alch_4.csv};
  \addlegendentry{Iteration 4: Sampling};

  \addplot[name path=line, draw=none] {0};

  \addplot fill between[
    of = alch_1 and line,
    split,
    every even segment/.style = {pattern color=blue!50, pattern=vertical lines},
    every odd segment/.style = {pattern color=blue!50, pattern=vertical lines},
    soft clip={domain=0:1},
  ];

  \addplot fill between[
    of = alch_2 and line,
    split,
    every even segment/.style = {pattern color=red!50, pattern=horizontal lines},
    every odd segment/.style = {pattern color=red!50, pattern=horizontal lines},
    soft clip={domain=0:1},
  ];

  \addplot fill between[
    of = alch_3 and line,
    split,
    every even segment/.style = {pattern color=brown!50, pattern=north east lines},
    every odd segment/.style = {pattern color=brown!50, pattern=north east lines},
    soft clip={domain=0:1},
  ];

\end{axis}
\end{tikzpicture}
\caption{Approximating the integral under the curve, hence calculating
$\Delta$G. The adaptive algorithm reevaluates the efficiency of the lambda
window mesh after every \SI{1}{\nano\second} and makes a decision whether to
place more lambda windows inside certain ranges. As we iterate every 1 ns,
the integral approximation becomes more accurate.}
\label{fig:adapt}
\end{figure}
