Computational drug campaigns require rapid screening of thousands of candidate 
compounds in order to design new therapeutics for clinical insight. Creating 
simulation protocols which have well defined uncertainty and produce 
statistically meaningful results represents a significant computational 
challenge. Differences amongst investigated systems will demand different 
protocols as studies progress. For example, drug design campaigns often require 
an initial screening of thousands of candidate compounds to filter out the worst 
binders before using more sensitive methods to refine the structure. Not all 
changes induced in protein shape or behavior are local to the drug binding site 
and, in some cases, simulation protocols will need to adjust to account for 
complex interactions between drugs and their targets within individual studies.

These complexities of protocol selection and refinements are captured using 
adaptive algorithms, which are algorithms whose dependencies are not know 
\textit{a priori} and are predicated on intermediate results. 
In bio-simulation several classes of adaptive algorithms exist which tackle 
different problem spaces. For example, advanced sampling is a subset of adaptive
sampling algorithms that guide simulations to explore and improve sampling 
quality by steering execution towards more interesting phase spaces (cite). 
Metadynamics and expanded ensemble dynamics are another class of well
established adaptive algorithms where individual simulations jump between 
simulation conditions. 

Drug campaigns which screen for thousands of compounds specifically require 
adaptive algorithms which can increasing the fidelity of statistical uncertainty
and thereby reduce time to convergence for a drug candidate. Adaptive quadrature 
algorithms deliver this requirement by adding additional simulations to reduce 
errors on the predicated binding affinity. Once the error threshold criteria is 
met for a candidate, the algorithm can halt simulations for that candidate while 
the next candidate is considered. 

Implementation of adaptive algorithms on high performance clusters (HPCs) are 
predicated on adaptive runtime systems that provide the ability to make runtime 
decisions based on intermediate results and can manage resources efficiently. 


As such producing timely clinical 
insight demands computational efficiency which is predicated upon advances in 
algorithms, scalable software systems and intelligent and efficient utilization 
of supercomputing resources. 



For molecular simulations to truly
influence decision making in industrial and clinical settings, the dual
challenges of scalability of drug candidates and enhancements in adaptive 
algorithms will need to be tackled. 
 


Most advances in high-performance computing have focused on the scale,
performance and optimization of single long simulations. Due to the end of
Dennard scaling, methodological advances and resource architectures (in 
particular GPGPUs), many applications are now
formulated using multiple, shorter ensemble-based simulations. Recent
development shows increasing interest in the use of molecular simulation to 
estimate the strength of macromolecular binding free energies~\cite{DeVivo2016}.

These methods in bio-molecular science have been developed to take advantage of 
the growing number of compute resources where multiple tasks are executed on 
these compute resources in parallel. Although these methods may suffice for 
existing problems, studying more drug candidates systems for longer durations 
will require sophistication in these algorithms. 
There are however, limited software solutions that can support the 
scalable execution of multiple tasks concurrently.



Both from the domain perspective and computational perspective there are 
deficiencies in throughput vs. high performance. the "fire and forget" mentality 
no longer fulfills computational efficiency when campaigns are limited by a 
computational budget or require rapid timely insight for clinical efficacy. 



% which provides a domain specific interface to leverage recent advances in
% abstracting the relatonships between building blocks in workflow systems to

Tools that facilitate the automated scalable and sophisticated computation of
varied binding free energy calculations on high-performance computing
resources are necessary. We have previously developed a tool for automating
system building and simulation input generation, the Binding Affinity
Calculator (BAC)~\cite{Sadiq2008}. More recently we have introduced the 
High-Throughput Binding Affinity Calculator (HTBAC)~\cite{dakka2017}, 
which brings advances in the design of domain-specific workflow systems 
using building blocks to facilitate the automation of deployment 
and execution of rapid binding affinities on leadership class machines. 


% In Dakka \textit{et al.}~\cite{dakka2017} we 
% demonstrated how HTBAC scales almost perfectly to hundreds of concurrent 
% multi-stage pipelines for a single protocol. 

%, which permits rapid time-to-solution that is essentially invariant
%of the size of candidate ligands.

% as well as the type and number of protocols concurrently employed.

%In this paper we aim to reproduce a collaboration project between UCL and
%GlaxoSmithKline to study a congeneric series of drug candidates binding to the
%BRD4 protein 
%(inhibitors of which have shown promising preclinical efficacy in
%pathologies ranging from cancer to inflammation)
%~\cite{Wan2017brd4}. This
%study compared two different protocols, known as TIES and
%ESMACS~\cite{Wan2017brd4, Bhati2017}, both based on an ensemble simulation
%philosophy. 


In this paper we reproduce results from a collaboration project between UCL
and GlaxoSmithKline to study a congeneric series of drug candidates binding to
the BRD4 protein (inhibitors of which have shown promising preclinical
efficacy in pathologies ranging from cancer to
inflammation)~\cite{Wan2017brd4}. These studies employed two different
protocols, known as TIES and ESMACS~\cite{Bhati2017}, both based on multiple
simulations of the same system.

% In this paper we reproduce results from a study of  congoneric series of
% componds which employed two different protocols, known as TIES and
% ESMACS~\cite{Wan2017brd4, Bhati2017}, both based on multiple simulations of
% the same system.

%In this approach multiple simulations are executed based on the
%same input system description, providing enhanced sampling and reduced time to
%completion. 
TIES is rigorous but computationally expensive and has a limited range of 
applicability.
ESMACS is approximate but can be employed across any set of ligands at lower 
computational cost.
%TIES is based on rigorous, but computationally expensive,
%calculations of relative free energies (i.e. results provide a comparison
%between two drugs). ESMACS, in contrast, provides absolute binding free
%energies at low computational cost, but to achieve this coarse grains many of
%the details of the system being studied. The simplifications employed by
%ESMACS can reduce its effectiveness in some systems. 
%In the real world
%application of these technologies, d
Drug design projects have limited resources,
%and must make trade-offs between the needs for rigour and coverage of a wide
%range of chemical space. 
%Initially large numbers of compounds must be screened
so initially large numbers of compounds must be cheaply screened
to eliminate poor binders (using ESMACS), 
%later 
before more accurate methods (such as
TIES) are needed as good binders are refined and improved. %This means that
%many projects will combine the use of both protocols.

%We explore the use of HTBAC for aforementioned sophisticated exploration of
%novel compounds binding to a target protein. 
In order to support such
investigations, in addition to being scalable, HTBAC must be enhanced to
support flexible resource reallocations schemes where resources can be moved
between simulations run using different protocols or systems, for example,
when one calculation has converged whilst another has not. This adaptability
makes it easier to manage complex programs where efficient use of resources
is required in order to achieve a time to completion of studies comparable to
those of high throughput chemistry. 

% \dwwnote{Could the next sentence go?}
% This functionality provides the
% foundations upon which we develop adaptable simulation schemes that
% automatically handle different system characteristics.




% In principle, many workflow management systems (WMS) can be used to express
% ensemble-based workflows to compute binding free energies. General-purpose WMS
% are however,  limited by a lack of specificity: workflow system with feature
% rich, but complex interface models impose substantial overhead when
% integrating application workflows in the runtime system, preventing users from
% quick and flexible applications prototyping.

% Furthermore, statistically meaningful results derived from simulations are of
% critical interest, as they can leverage additional sampling of simulations
% which could lead to an improvement in precision of free energy calculations.
% Such decisions typically, cannot be made {\it a priori} and require postmortem
% user intervention in order to analyze and spawn additional simulations.
% Specific features such as steering and adaptivity (e.g., spawning additional
% simulations) during runtime often require a redistribution of resources.
% Current WMS do not provide adequate support for dynamic resource utilization
% and require special extensions for adaptive formulations thus limiting the
% possibility of adaptive workflows.


In this work we demonstrate the use of HTBAC to adaptively run both protocols,
including mixed protocol runs. 
Calculations using either protocol require simulations of free and 
protein bound ligand necessitating the coordination of runs with 
heterogeneous computational requirements.
%Executing either protocol often involves
%running simulations of both free and  bound  ligand, with the later involving
%a much larger system than the former. This means that even single calculations
%must coordinate simulations with heterogeneous computational requirements.
Further to the use of a common framework to improve the ease of deployment and
efficiency of execution of existing protocols we show how HTBAC can aid the
development of enhanced approaches. 
%We demonstrate this capacity through the development of a variant of TIES 
%employing adaptive sampling which improves time to convergence.

%The TIES protocol is highly sensitive to
%the chemical details of the compounds being studied, which means that
%different runs may require different sampling strategies to achieve optimal
%time to convergence. We have developed an adaptive variant of TIES which
%automatically increases the sampling in areas where it is needed for each
%system, allowing more rapid convergence of calculations. Furthermore, this
%approach facilitates a data driven approach to future protocol refinement.

%This approach not only allows us to tailor the protocol to the particular
%system but to provide meaningful statistical uncertainties for all of our
%models. 
These developments fit into a wider vision in which the use of
flexible and responsive computational protocols produce accurate,
precise and reproducible estimates of the free energy of binding with 
meaningful error bars. Not only would this allow for wider uptake of 
computational techniques in industrial settings but opens up possibilities 
of using these technologies in clinical decision support scenarios. By creating 
a `digital twin', where the target protein is based on the real patients 
genetic sequence, a specific individuals response to different 
treatments could be predicted. 

% \dwwnote{Could remove the next sentence}
% This approach would be
% applicable even in the case of rare variants where insufficient data is
% available for statistical methods to be informative.

The novel contributions of HTBAC are: (i) Unprecedented throughput: it allows
the concurrent screening for drug binding affinities of multiple compounds at
unprecedented scales, both in the number of candidates and resources utilized;
(ii) Agile selection of different binding affinity protocols: HTBAC supports
inter-protocol adaptivity, leading to resources being assigned at runtime to
the ``optimal" protocol (as determined by accuracy for given computational
cost); (iii) Support for intra-protocol adaptivity, which provides the
efficient execution of individual protocols. 

% In future, we will demonstrate
% how the above set the stage for design of experiments that will allow for
% optimization of the collective and overall time-to-insight as opposed to any
% single simple calculation.

%\dwwnote{Do we need the next paragraph? A slightly shortened version is: }

In this paper we describe the background to our binding free energy protocols
and how we designed HTBAC to meet the challenges faced in bringing this
approach up to extreme scale. We then describe a set of experiments
characterizing the performance and scalability of HTBAC and the development of
an adaptive version of TIES. 

% In the next section, we review previous work using ensemble molecular dynamics
% and outline the challenges faced in bringing this approach up to extreme
% scale. Section 3 provides details of the TIES and ESMACS protocols and the
% BRD4 system we will apply them to. In Section 4, we discuss how HTBAC has
% been designed and implemented in order to meet the computational challenges
% associated with the scalable execution of multiple, and possibly concurrently
% executing protocols. In Sections 5 and 6 we describe the design and then
% results of a series of experiments characterizing the performance and
% scalability of HTBAC on the Blue Waters and Titan supercomputers. We conclude
% with a discussion of the impact of HTBAC, implications for binding affinity
% calculations and near-term future work.
