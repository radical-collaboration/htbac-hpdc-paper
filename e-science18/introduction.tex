Drug discovery and design is immensely expensive with one study putting the
current cost of each new therapeutic molecule that reaches the clinic at
US\$1.8 billion~\cite{Paul2010}. A diversity of computational approaches,
specifically binding free energy calculations which rely on physics-based
molecular dynamics simulations (MD) have been developed~\cite{Mobley2012}
and blind tests show that many have considerable predictive
potential~\cite{Mey2017,Yin2017}. The development of commercial approaches
that claim accuracy of below 1 kcal mol$^{-1}$~\cite{Wang2015} has led to
increased interest from the pharmaceutical industry~\cite{Ganesan2017} in
designing computational drug campaigns.

These improvements can be attributed to many advances in methodologies and
hardware. Specifically, ensemble-based binding free energy calculations,
which favor many shorter simulation trajectories over few longer simulations,
have been shown to increase sampling efficiency whilst also reducing time to
insight~\cite{weis_ligand_2006}. For binding affinity calculations to gain
traction, they must have well-defined uncertainty and consistently produce
statistically meaningful results.

Computational drug campaigns rely on rapid screening of millions of
compounds, which start with an initial screening of candidate compounds to
filter out the ineffective binders before using more sensitive methods to
refine the structure of promising candidates. Two prominent ensemble-based
free energy protocols, ESMACS and TIES~\cite{Bhati2017}, have shown the
ability to consistently filter and refine the drug design process. The ESMACS
(enhanced sampling of molecular dynamics with approximation of continuum
solvent) protocol provides an ``approximate'' endpoint method used to screen
out poor binders. The TIES protocol (thermodynamic integration with enhanced
sampling) uses the more rigorous ``alchemical'' thermodynamic integration
approach as implemented in NAMD \cite{Phillips2005, Straatsma1991}. These
protocols have produced statistically meaningful results for industrial
computational drug campaign~\cite{Wan2017brd4}.

In recent years, considerable effort has been put into improving the
efficiency of free energy calculations~\cite{Klimovich2015, Naden2016,
Kaus2013}. As drug screening campaigns can cover millions of compounds and
require hundreds of millions of core-hours, it is important that these
calculations be effective and aim to optimize the accuracy and precision of
results. This is challenging as, by definition, drug discovery involves
screening compounds that are highly varied and potentially unique in their
chemical properties. The variability in the drug candidate chemistry results
in diverse sampling behavior that contributes to the statistical uncertainty
of binding free energy predictions. Further, a particular difficulty comes
from the fact that not all changes induced in protein shape or behavior are
local to the drug binding site and, in some cases, simulation protocols will
need to adjust to account for complex interactions between drugs and their
targets within individual studies.

Traditionally, simulation protocols have been deployed with the worst case
scenario and with sufficient sampling conducted to account for the slowest
convergence previously encountered. This approach has at least two
shortcomings: it potentially wastes valuable computational resources  and
fails to account for the different value of the simulations results. For
example, in a drug campaign it is more important to understand how strong is
the binding of the best compound candidates than precisely know how weak is
the interactions of the worst compound.

Key to successful campaigns is identifying when small chemical changes result
in large binding strength changes. This can mean that the parameters which
are important to campaigns evolve as the study progresses. Here we show how
adaptive approaches within ensemble-based free energy protocols can be
designed to capture unique chemical properties and thereby customize the
simulations for a candidate to make the most effective use of computational
resources, delivering better fidelity of statistical uncertainties than
general approaches.

Implementation of adaptive algorithms on high performance computers (HPC)
require runtime systems that support adaptivity, i.e., provide the ability to
make runtime decisions based on intermediate results and can manage resources
efficiently. To achieve scalability and efficiency, such adaptivity cannot be
performed via user intervention and hence automation of the control logic and
execution is important. We have developed the High-Throughput Binding
Affinity Calculator (HTBAC) to enable the scalable execution of adaptive
algorithms.

This paper identifies the challenge in advancing adaptive methodologies. It
makes three main contributions: (1) shows the importance of using adaptive
approaches within ensemble-based free energy protocols to improve binding
affinity accuracy given a fixed amount of computing resources; (2) presents
and characterizes HTBAC, a software system that enables the scalable
execution of adaptive applications; and (3) shows the capability to execute
adaptive applications at scale, validating their scientific results.

This paper is organized as follows: Section~\ref{sec:science-drivers}
introduces ESMACS and TIES, two ensemble-based free energy protocols, arguing
how implementing adaptive methodology within TIES could achieve higher
precision with limited resources. Section~\ref{sec:related-work} describes
the motivation for ensemble-based approaches and existing solutions alongside
the limitations in their ability to support adaptive methods.
Section~\ref{sec:htbac} describes the design and implementation of HTBAC and
how we used HTBAC to implement an adaptive and nonadaptive version of TIES.
In Section~\ref{sec:experiments}, we present experiments we performed with
HTBAC to characterize its scalability and overheads, and showing that given a
fixed amount of computing resources, we can achieve better accuracy and
better time to solution using adaptive methods.