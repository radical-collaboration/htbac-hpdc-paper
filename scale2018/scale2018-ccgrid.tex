\documentclass[conference]{IEEEtran}
\input{head}
\input{include}

\begin{document}


% \title{Alchemical and Endpoint Free Energy Calculations at Scale}

%\title{Rapid, Concurrent and Adaptive Extreme-scale Free energy calculation}

\title{Trading-off Accuracy with Computational Cost: Adaptive Algorithms to Reduce Time to Clinical Insight}



% \author{Jumana Dakka$^{1}$,  Kristof Farkas-Pall$^{3}$, David W. Wright$^{3}$, .... Shantenu Jha$^{1}$$^{,2}$, \\

\author{Jumana Dakka$^{*,1}$, Kristof Farkas-Pall$^{*,2}$, Vivek Balasubramanian$^{1}$ , Matteo Turilli$^{1}$, \\
 Shunzhou Wan$^{2}$, David W Wright$^{2}$, Stefan Zasada$^{2}$, \\\
 Peter V Coveney$^{2}$, Shantenu Jha$^{1,3}$ \\

  \small{\emph{$^{1}$ Rutgers, the State University of New Jersey, Piscataway, NJ 08854, USA}}\\
   \small{\emph{$^{2}$ University College London, London, UK, WC1H 0AJ}}\\
   \small{\emph{$^{3}$ Brookhaven National Laboratory, Upton, New York, 11973}}\\
   \small{\emph{$^{*}$ Contributed Equally}}
}


\date{}
\maketitle

\begin{abstract}

\end{abstract}


% ---------------------------------------------------------------------------
% Introduction
% ---------------------------------------------------------------------------
\section{Introduction}\label{sec:intro}

The efficacy of drug treatments depends on how tightly small molecules bind to
their target proteins. Quantifying the strength of these interactions (the so
called ‘binding affinity’) is a grand challenge of computational chemistry,
the surmounting of which could revolutionize drug design and provide the
platform for patient specific medicine. Recently, evidence from blind
challenge predictions and retrospective validation studies has suggested that
molecular dynamics (MD) can now achieve useful predictive accuracy (1
kcal/mol)\cite{shirts-mobley-chodera:2007:annu-rep-comput-chem:prime-time,
abel:jacs:2015:fep-plus}. This accuracy is sufficient to greatly accelerate
lead optimization \cite{shirts-mobley-brown:2009:sbdd}. Exploiting these
advances and further refining the technologies involved requires the
marshaling of huge simulation campaigns, and impacting clinical or industrial
decision making means that computations must be turned around in timescales of
hours or days. In this SCALE challenge, we demonstrate the use of a framework
--- HTBAC, designed to support the computational requirements of high-
throughput drug binding affinity calculations. HTBAC facilitates the execution
of the numbers of simulations while supporting the adaptive execution of
algorithms. Furthermore, HTBAC  enables the selection of experimental
parameters during runtime, which can in principle optimize the use of
computational resources whilst producing results with a target uncertainty.


% ---------------------------------------------------------------------------
% Scientific Motivation
% ---------------------------------------------------------------------------

\subsection{Scientific Motivation}\label{sec:motivation}

This work is motivated by the real world problem of providing clinical insight
from drug candidate data on a time scale that is as short as possible,
Specifically, we will demonstrate the scalability and flexibility of HTBAC by
reproducing results from a collaboration project between UCL and
GlaxoSmithKline to study a congeneric series of drug candidates binding to the
BRD4 protein (inhibitors of which have shown promising preclinical efficacy in
pathologies ranging from cancer to inflammation) ~\cite{Wan2017brd4}. These
studies employed two different protocols, known as TIES and ESMACS
~\cite{Bhati2017}, both based on multiple simulations of the same system. Drug
design projects have limited resources, so initially large numbers of
compounds must be cheaply screened to eliminate poor binders (using ESMACS),
before more accurate methods (such as TIES) are needed as good binders are
refined and improved.

In order to support such investigations, in addition to being scalable, HTBAC
must be enhanced to support flexible resource reallocations schemes where
resources can be moved between simulations run using different protocols or
systems, for example, when one calculation has converged whilst another has
not. This adaptability makes it easier to manage complex programmes where
efficient use of resources is required in order to achieve a time to
completion of studies comparable to those of high throughput chemistry.

Bromodomain-containing proteins, and in particular the four members of the BET
(bromodomain and extra terminal domain) family, are currently a major focus of
research in the pharmaceutical industry. Small molecule inhibitors of these
proteins have shown promising preclinical efficacy in pathologies ranging from
cancer to inflammation. Indeed, several compounds are progressing through
early stage clinical trials and are showing exciting early
results~\cite{Theodoulou2016}. One of the most extensively studied targets in
this family is the first bromodomain of bromodomain-containing protein 4
(BRD4-BD1) for which extensive crystallographic and ligand binding data are
available~\cite{Bamborough2012}.

\begin{figure}
  \centering
  \includegraphics[width=0.60\columnwidth]{./egfr.png}
  \caption{Cartoon representation of the EGFR kinase bound to the inhibitor
  AEE788 shown in chemical representation (based on PDB:2J6M). Two residues
  implicated in modulating drug efficacy are highlights; in pink T790 and in
  orange L858. Mutations to either of these residues significantly alter the
  sensitivity to TKIs.}\label{fig:egfr}
\end{figure}


We have previously investigated a congeneric series of ligands binding to
BRD4-BD1 (we shall from now on refer to this are simply BRD4) using both
ESMACS and TIES. This was performed in the context of a blind test of the
protocols in collaboration with GlaxoSmithKline~\cite{Wan2017brd4}. The goal
was to benchmark the ESMACS and TIES protocols in a realistic drug discovery
scenario. In the original study, we investigated chemical structures of 16
ligands based on a single tetrahydroquinoline (THQ) scaffold. % ~\cite{Gosmini2014}. 
Here we focus on the first seven of these ligands to test
and refine the protocols used and the way in which they were executed. The
results of our previous work provide a benchmark of both accuracy and
statistical uncertainty to which we can compare our new results.

\subsection{Binding Affinity Calculation Protocols}\label{sec:bac}

Computing accurate protein-drug binding affinities (also known as binding free energies) requires a simulation technique which captures the chemical detail of the system. 
MD simulations are the time dependent numerical integration of the classical equations of motion for molecular systems. 
Application of MD to atomistic models of proteins and their ligands can be used to answer questions about the properties of a specific system often more readily than experiments on the actual system. 
Free-energy calculations in the framework of MD simulations not only yield quantitative estimates of binding strength but also provide insights into the most important interactions driving the process.

Most methods for calculating binding affinities fit into one of two broad classes; so called alchemical and endpoint methodologies. 
Alchemical free energy calculations employ unphysical (“alchemical”) intermediates to calculate changes in free energies between two systems. 
It is common in these methods to refer to a variable, $\lambda$, which describes the path taken to transform one protein sequence (or ligand) into another. 
Endpoint methods, as the name suggests, consider the difference in energy between bound and unbound structures. 
To obtain information on the differences in binding affinity of different sequences for a panel of kinase inhibitors requires a deployment of various strategies, incorporating both alchemical and endpoint approaches. 
In this work we deploy approaches from both of these classes.

\subsection{Ensemble Molecular Dynamics}\label{sec:emd}

Statistical mechanics provides the prescription for calculating such macroscopic quantities as ensemble averages of microscopic states. Traditionally, these macroscopic properties have usually been calculated from the time average of a single “long” duration trajectory. An intuitive and potentially more time efficient method to capture the mixing dynamics required to describe an equilibrium thermodynamic state is the use of an ensemble of separate trajectories. \cite{Coveney2016}

The major sources of error in free energy calculations are the representation of the system chemistry encoded in the forcefield used, finite sampling and the free energy estimator. Protocols developed in the Coveney labs have obtained accurate and precise results which successfully reproduce experimental binding free energies from a wide range of systems. \cite{Wright2014, Wan2017brd4, Wan2015, Wan2011} 
Comparisons of results obtained for a large set of sequences will provide valuable insights on the importance of choices made in simulation and analysis for the overall accuracy and predictive power of free energy calculations, and facilitate the refinement of our protocols.


\subsection{Alchemical Protocol (TIES)}\label{sec:ties}

Alchemical methods employ MD simulations of unphysical, alchemical intermediate states that attenuate the interactions of the small molecule with its environment. 
These alchemical intermediate states include both the fully-interacting complex as well as replicas in which the ligand does not interact with its environment, and allow the total free energy of binding—including entropic and enthalpic contributions—to be efficiently computed. Typically, the alchemical path between the states of interest is described by a parameter, $\lambda$, which varies between 0 for the initial and 1 for the final state of the transformation of interest. 
Sampling is then performed at a series of points along this path and the gradient of the energy integrated to calculate the binding affinity.
Simulations conducted at a given $\lambda$ value are said to be sampling a $\lambda$ window at that point.

The TIES (thermodynamic integration with enhanced sampling) protocol, developed within the Coveney lab, employs ensemble sampling at each $\lambda$ window to yield reproducible, accurate, and precise relative binding affinities. 
\cite{ Wan2017brd4} Based on the direct calculation of ensemble averages, it allows us to determine statistically meaningful results along with complete control of errors. 
As currently designed, TIES computes the change in binding affinity between two related system (termed ‘relative binding affinities’).


\subsection{Endpoint Protocol (ESMACS)}\label{sec:esmacs}

Computationally cheaper, but less rigorous methods, endpoint methods have been used to directly compute the binding strength of a drug to the target protein from MD simulations (as opposed to differences in affinity). 

We have developed an ensemble-based endpoint protocol called ESMACS (enhanced sampling of molecular dynamics with approximation of continuum solvent). The protocol is built on the popular molecular mechanics Poisson–Boltzmann surface area (MMPBSA) \cite{Massova1999} method which makes a continuum approximation for the aqueous solvent in order to obtain results on practical timescales. Commonly, MMPBSA analyses are performed on a single MD trajectory, or even a single structure. We have demonstrated the lack of reproducibility of such an approach in both HIV-1 protease and MHC systems, with calculations for the same protein-ligand combination, with identical initial structure and force field, shown to produce binding affinities varying by up to 12 kcal/mol for small ligands (flexible ligands can vary even more). \cite{Wan2015} ESMACS employs MMPBSA to produce ensemble- based, converged and reproducible, determinations of binding free energies (separate ligand and receptor trajectories can also be used to account for adaptation energies). This provides a rapid quantitative approach sensitive enough to determine changes in binding free energies which differentiate susceptible and resistant sequences (typically of the order of 2 kcal/mol).

% ---------------------------------------------------------------------------
% Section II
% ---------------------------------------------------------------------------
\section{BRD4 System}\label{sec:system_description}

Initial coordinates for the protein-ligand system were taken from the X-ray
crystal structure PDB ID: 4BJX.
% ~\cite{Wyce2013}. 
This structure contains a
ligand based on the THQ template and other ligands were alligned with this
common scaffold. Preparation and setup of the simulations were implemented
using our automated tool, BAC~\cite{Sadiq2008}. This process including
parametrization of the compounds, solvation of the complexes, electrostatic
neutralization of the systems by adding counterions and generation of
configurations files for the simulations. The AMBER ff99SB-ILDN
% ~\cite{Lindorff-Larsen2010} 
force field was used for the protein, and TIP3P was used
for water molecules. Compound parameters were produced using the general AMBER
force field (GAFF)~\cite{Wang2004} with Gaussian 03
%~\cite{Frisch} 
to optimize compound geometries and to determine electrostatic potentials at
the Hartree–Fock level with 6-31G** basis functions. The restrained
electrostatic potential (RESP) module in the AMBER package
%~\cite{Case2005} 
was used to calculate the partial atomic charges for the compounds. All systems
were solvated in orthorhombic water boxes with a minimum extension from the
protein of 14 \AA\ (the resulting systems contain approximately 40 thousand
atoms).

% ---------------------------------------------------------------------------
% Section III
% ---------------------------------------------------------------------------
\section{Performance Metrics}\label{sec:performance}

TIES is rigorous but computationally expensive and has a limited range of
applicability; ESMACS is approximate but can be employed across any set of
ligands at lower computational cost. Both protocols simulates a large range
of mutations where each mutation is repreesnted as an independent set of
pipelines. Each pipeline is composed of multiple replicas, i.e., a sequence
of simulations and a single analysis. This sequence is repeated over a number
of iterations, depending on the convergence criteria of the analysis. 

All the replicas of both TIES and ESMACS requires 16 cores, where each
pipeline runs approximately 10 hours, over a range of 100 mutations where
each mutations spawns either 25 or 65 replicas, depending on the
protocol.\mtnote{We need a better description of TIES and ESMACS.}

\mtnote{Is the title of the section wrong? I see no mention of performance
metrics.}


% ---------------------------------------------------------------------------
% Section IV
% ---------------------------------------------------------------------------
\section{Computational Challenges}\label{sec:cc}

\jhanote{There is no mention of adaptivity as a challenge. Vivek should add
from his paper.}

\jhanote{Need better organization. Separate out challenge of (i) simple and usable software systems, (ii) scale and interoperability, and (iii) adaptivity challenges. }

High-performance computing (HPC) environments were designed to primarily
support the execution of single simulations. Current HPC platforms enable the
strong and weak scaling of single tasks (hitherto mostly simulations), with
limited software and systems support for the concurrent execution of multiple
heterogeneous tasks as part of a single application (or workflow). As the
nature of scientific inquiry and the applications to support that inquiry
evolve, there is a critical need to support the scalable and concurrent
execution of a large number of heterogeneous tasks.

Sets of tasks with dependencies that determine the order of their execution
are usually referred to as ``workflows''. Often times, the structure of the
task dependencies is simple and adheres to discernible patterns, even though
the individual tasks and their duration are non-trivially distinct. Put
together, it is a challenge to support the scalable execution of workflows on
HPC resources due to the existing software ecosystem and runtime systems
typically found.

Many workflow systems have emerged in response to the aforementioned problem.
Each workflow system has its strengths and unique capability, however each
system typically introduces its problems and challenges. In spite of the many
successes of workflow systems, there is a perceived high barrier-to-entry,
integration overhead and limited flexibility.

Interestingly, many commonly used workflow systems in high-performance and
distributed computing emerged from an era when the software landscape
supporting distributed computing was fragile, missing features and services.
Not surprisingly, initial workflow systems had a monolithic design that
included the end-to-end capabilities needed to execute workflows on
heterogeneous and distributed cyberinfrastructures. Further, these workflow
systems were typically designed by a set of specialists to support large
``big science'' projects such as those carried out at the
LHC~\cite{breskin2009cern} or LIGO~\cite{althouse1992ligo}. The fact that the
same workflow would be used by thousands of scientists over many years
justified, if not amortized, the large overhead of integrating application
workflows with monolithic workflow systems. This influenced the design and
implementation of interfaces and programming models.

However as the nature, number and usage of workflows has evolved so have the
requirements: scale remains important but only when delivered with the
ability to prototype quickly and flexibly. Furthermore, there are also new
performance requirements that arise from the need to support concurrent
execution of heterogeneous tasks. For example, when executing multiple
homogeneous pipelines of heterogeneous tasks, for reasons of efficient
resource utilization there is a need to ensure that the individual pipelines
have similar execution times. The pipeline-to-pipeline fluctuation must be
minimal while also managing the task-to-task runtime fluctuation across
concurrently executing pipelines.

% \mtnote{Should we define what an ensemble is?} \jdnote{We defined ensemble
% in the Methodology Section: We term this approach ensemble molecular
% dynamics, “ensemble” here referring to the set of individual (replica)
% simulations conducted for the same physical system.}

Thus the flexible execution of heterogeneous ensembles MD simulations face
both system software and middleware challenges: existing system software that
is typically designed to support the execution of single large simulations on
the one hand, and workflow systems that are designed to support specific use
cases or `locked-in' end-to-end executions. In the next Section, we discuss
the design and implementation of the RADICAL-Cybertools, a set of software
building blocks that can be easily composed to design, implement and execute
domain specific workflows rapidly and at scale.


Adaptive applications focus on leveraging intermediate data to 
adapt the application to study larger problems, longer time scales and to 
engineer better fidelity in the modeling of complex phenomena. Enabling these
capabilities on HPC systems can be beneficial to domain science applications 
but comes with its own set of challenges in expressibility, instantiation and 
implementation.

The first challenges lies in the expressibility of adaptive applications. 
Composition of adaptive applications requires APIs that enable the expression of
the application without adaptivity and specification of methods that capture how
the application adapts to intermediate data. The former translates to a 
description of the application task graph, which consists of all tasks of the 
application and their interdependencies such that there exists a specific
order of execution of the tasks, while the latter specifies adaptive 
methods that adapt this task graph.

The second challenge lies in determining when the adaptation is instantiated. 
The adaptation is described at the end of the execution of a task wherein a new
task graph is generated. Different strategies can be employed in the 
instantiation of the adaptation such as abort and rollback, proceed, forward
recovery~\cite{van2000dealing}.

The third challenge lies in implementation of the adaptation of the application
during runtime. We divide this challenge into three parts: (i) propagation of
adapted task graph to all components, (ii) consistency of the state of task
graph between different components and (iii) Minimal overhead: The overhead of 
performing adaptive operations should be minimal compared to the execution 
duration of the tasks.

\section{Solution}\label{sec:solution}

%\jhanote{The Solution is not just HTBAC per se, but the application of HTBAC to the specific problem. The specific problem has not been identified so far. I would propose divided this section into two parts. The first about  HTBAC and its scalability; and the second part about its application to the specific problem.}

The RADICAL Cybertools (RCT), developed by The RADICAL Lab, enables the
efficient and dynamic execution of ensembles on heterogeneous computing
resources. Different from other runtime systems, RCT decouples the workload
execution and resource management details from the expression of the
application, which significantly reduces the burden on the end user.
RCT has been used extensively to support
biomolecular sciences methods and algorithms, e.g., replica exchange, adaptive
sampling and high throughput binding affinity calculations.

Here we describe High Throughput Binding Affinity Calculator (HTBAC),
which builds upon the RADICAL Cybertools, as the framework solution to support
the coordination of the required scale of computations, thereby
allowing us to employ thousands of cores at a time.

Most benchmark evaluations of free energy protocols in the literature look
at only a small number of systems, drawing inferences from tens or hundreds
of runs. HTBAC facilitates studies on unprecedented scales, with the number
of systems investigated an order of magnitude larger than published studies,
which provides the opportunity to gain invaluable knowledge on the domain of
applicability of current MD technologies. In particular, we demonstrate the
use of HTBAC to compute the binding affinities of anti-cancer drugs to their
target proteins (the EGFR kinase) using two simulation protocols with differing
levels of rigor and computational cost, ESMACS and TIES.

HTBAC is not limited to these protocols as additional protocols can be
expressed and implemented easily with the HTBAC user-facing API, however for
demonstration we focus on these ensemble protocols HTBAC has demonstrated
sizable execution and performance of the ESMACS~\cite{dakka2017} and TIES~\cite{dakka_farkaspall} protocols on leadership
class machines including NCSA Blue Waters. For example, we demonstrated how HTBAC
scales almost perfectly to hundreds of concurrent multi-stage pipelines for the TIES 
protocol in ~\ref{fig:weak_scaling}.

\begin{figure}
  \centering
   \includegraphics[width=\columnwidth]
   {weak_scaling_TIES_instances_50,000_timesteps_with_16_instances.pdf}
  \caption{Weak scaling properties of HTBAC. We investigate the
  weak scaling of HTBAC as the ratio of the number of protocol instances to
  resources is kept constant. Overheads of HTBAC framework (right), and RCT overhead 
  (left) and total execution time \(TTX\) (left) for experimental configurations investigating the 
  weak scaling of TIES. We ran two trials for each protocol instance 
  configuration. Error bars in \(TTX\) in 2 and 8-protocol runs are 
  insignificant.}
\label{fig:weak_scaling}
\end{figure}

Both ESMACS and TIES have been successfully used to predict binding affinities
quickly and accurately. 
In their standard non-adaptive forms TIES is approximately 2.5 times more expensive
than ESMACS.
The ligands investigated here are closely related so it could be expected that the
greater accuracy of TIES would be required to differentiate them.
However, in a drug design scenario many drug candidates would need to be investigated
meaning that optimizing the execution time while retaining (or improving) accuracy is
desirable. 
Given the very large number of drug candidates, it is imperative to
gain maximum insight into potential candidate compounds using time and
resources efficiently. 
This provides one clear motivation for the use of
adaptive methods which minimize the compute time used whilst producing binding
free energy estimates meeting pre-defined quality criteria (such as
convergence or statistical uncertainty below a given threshold).


Typically, datasets will involve ligands with a wide range of chemical 
properties which can impact not only the time to convergence, but the type 
of sampling required to gain accurate results.
In general, there is no way to know before running
calculations exactly which setup of calculation is required for a particular
system. 
Even within closely related ligands, such as the THQ based BRD4 ligands 
studied here, the $\partial U/\partial\lambda$ curve$\lambda$ integrated in TIES 
varies between physical systems (drugs). 
Consequently, the number (or the exact location) of the
$\lambda$ windows that will most impact the calculation are not known
\textit{a priori}, and change between systems. 
As multiple
simulations must be run for each window, sampling with a very high frequency
is expensive and impractical. Furthermore, adaptive placement of $\lambda$
windows is likely to better capture the shape of the
$\partial U/\partial\lambda$ curve, leading to more accurate and precise
results for a given computational cost (see figure~\ref{fig:adaptive}).

\input{adaptive_plot}

In order to support such
investigations, in addition to being scalable, HTBAC is enhanced to
support flexible resource reallocations schemes where resources can be moved
between simulations run using different protocols or systems, for example,
when one calculation has converged whilst another has not. This adaptability
makes it easier to manage complex programs where efficient use of resources
is required in order to achieve a time to completion of studies comparable to
those of high throughput chemistry.

The novel contributions of HTBAC are: (i) Unprecedented throughput: it allows
the concurrent screening for drug binding affinities of multiple compounds at
unprecedented scales, both in the number of candidates and resources utilized;
(ii) Agile selection of different binding affinity protocols: HTBAC supports
inter-protocol adaptivity, leading to resources being assigned at runtime to
the ``optimal" protocol (as determined by accuracy for given computational
cost); (iii) Support for intra-protocol adaptivity, which provides the
efficient execution of individual protocols.



% While RCT supports concurrent task execution up to 000 tasks on Blue Waters,
% the user is required to translate and codify each of these protocols with the
% corresponding mutation using the EnTK API, and scale these protocols
% based on the desired number of mutations and replicas [9]. We leverage
% the advanced resource management capabilities of RADICAL Cybertools and
% automate this process by providing a native selection of protocols where
% the user only provides the configuration files for each mutation, the number
% of replicas per mutation and the number of cores needed to support each
% protocol. To this effect, the framework uses EnTK to provide a higher level of
% abstraction that allows the user to interface closer to the science problem.


\section{Impact of Solution}\label{sec:impact}


The flexibility provided by HTBAC to run adaptive workflows offers huge advantages scientifically.
Firstly the intra-protocol adaptivity allows the automated optimization of calculations to ensure that
results are obtained with known precision across systems which may exhibit very different behavior (for example levels of `roughness' in the $\partial U/\partial\lambda$ curve in TIES).
This has a significant impact of the reliability of comparisons between runs.
The ability to switch between protocols on the other hand offers a mechanism through which `cheaper' approximate methods (such as ESMACS) can be used to scan large regions of chemical space, whilst more accurate and `expensive' ones are employed to investigate areas of specific interest (TIES).
This maps directly onto processes such as hit to lead optimization in drug discovery and cold be of particular use in investigating activity cliffs.
This is a phenomena where small chemical changes provide large differences in drug efficacy.
If changes are detected using an approximate method it is important to verify that they come from
real chemical effects and not simply inaccuracies in the computational algorithm employed.

The scale enabled by HTBAC also has an impact on the potential for scientific discovery using free energy calculations.
Most studies in the literature are limited to the investigation of tens of protein-ligand complexes.
In order to establish the validity of particular combinations of forcefield and simulation protocol and
quantify uncertainties much larger campaigns are needed.
Our ensemble has already provided evidence that the variability in single runs is sufficient to
swamp true differences between systems of interest.
The combined need for both large numbers of systems and multiple repeats of each one produces a requirement
for middleware to manage huge numbers of simulations.

HTBAC allows the concurrent screening
for drug binding affinities of multiple compounds at unprecedented scales,
both in the number of candidates and resources utilized. Specifically, we
investigated weak scaling behavior for screening sixteen drug candidates
concurrently using thousands of multi-stage pipelines on more than 32,000
cores on NCSA Blue Waters. This permits a rapid time-to-solution that is
essentially invariant with respect to the calculation protocol,
size of target system and number of ensemble simulations. In addition,
HTBAC enabled the adaptive execution of the TIES protocol
providing greater convergence (i.e., lower errors) for a given amount of
computational resources.

These developments fit into a wider vision in which the use of
flexible and responsive computational protocols produce accurate,
precise and reproducible estimates of the free energy of binding with
meaningful error bars. Not only would this allow for wider uptake of
computational techniques in industrial settings but opens up possibilities
of using these technologies in clinical decision support scenarios. By creating
a `digital twin', where the target protein is based on the real patients
genetic sequence, a specific individuals response to different
treatments could be predicted.


\section{Analysis of Solution}\label{sec:analysis}

Our previous work deploying both ESMACS and TIES has typically involved comparing 10 to 20 computed
binding affinities.
An example would be the recent study of BRD4 inhibitors conducted in collaboration by UCL and GSK \cite{Wan2017brd4}, which involved 15 drugs (usually we prepare a similar number of systems using each protocol, here 12 TIES transformations were studied).
Using our non-adaptive protocols each system typically requires approximately 10k and 25k core hours for ESMACS and TIES respectively.
So a 20 compound study (with 20 transforms) would use $\sim$700k core hours.
As previously described this type of study does not meet the size necessary to really impact industrial processes which focus from libraries of potentially millions of compounds down to hundreds of hits which need to be optimized.
HTBAC is designed to enable this types of workflow to be sustained, a development which means that we can now look to conduct the scale of studies necessary to refine protocols and forcefields for production work in drug discovery and refinement.



% ---------------------------------------------------------------------------
% Demonstration
% ---------------------------------------------------------------------------
\section{Demonstration}\label{sec:demo}


% ---------------------------------------------------------------------------
% BIBLIOGRAPHY
% ---------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{rutgers,ucl,mskcc}

\end{document}
